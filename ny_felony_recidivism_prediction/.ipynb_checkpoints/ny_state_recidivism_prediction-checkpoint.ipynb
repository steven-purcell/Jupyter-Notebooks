{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\".\\\\Recidivism__Beginning_2008.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_labels = data['County of Indictment'].astype('category').cat.categories.tolist()\n",
    "replace_county = {'County of Indictment' : {k: v for k,v in zip(county_labels,list(range(1,len(county_labels)+1)))}}\n",
    "\n",
    "gender_labels = data['Gender'].astype('category').cat.categories.tolist()\n",
    "replace_gender = {'Gender' : {'FEMALE': 0, 'MALE': 1}}\n",
    "\n",
    "replace_status = {'Return Status':{'Returned Parole Violation': 1, 'New Felony Offense': 1, 'Not Returned': 0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_dummies = data.copy()\n",
    "data_w_dummies.replace(replace_county, inplace=True)\n",
    "data_w_dummies.replace(replace_gender, inplace=True)\n",
    "data_w_dummies.replace(replace_status, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_w_dummies = data_w_dummies.drop('Release Year', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data_w_dummies.iloc[:,0:-1]\n",
    "y_data = data_w_dummies.iloc[:,-1:]\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(x_data, y_data, test_size=.20,\n",
    "                                                    random_state=numpy.random.randint(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandForest = RandomForestClassifier()  # Create Random Forest estimator object\n",
    "GradBoost = GradientBoostingClassifier()\n",
    "classifier_list = [GradBoost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'AUC': 0.5359730305528906,\n",
      "    'Accuracy': 0.6075734282708957,\n",
      "    'Classifier': 'GradientBoostingClassifier',\n",
      "    'Confusion Matrix': '[[13506.  1147.]\\n [ 8553.  1512.]]',\n",
      "    'F1-Score': 0.23766111285759198,\n",
      "    'False Negative': 8553,\n",
      "    'False Positive': 1147,\n",
      "    'Precision': 0.5686348251222264,\n",
      "    'Recall': 0.15022354694485843,\n",
      "    'Sensitivity': 0.9217225141609227,\n",
      "    'Specificity': 0.15022354694485843,\n",
      "    'True Negative': 13506,\n",
      "    'True Positive': 1512}\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifier_list:\n",
    "        clf = classifier\n",
    "\n",
    "        scores, mean_auc, mean_tpr, mean_spec, mean_prec, mean_rec, mean_f1 = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        cm0 = numpy.zeros((2, 2))\n",
    "\n",
    "        fit_data = clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_cv)\n",
    "\n",
    "        '''Calculate performance metrics'''\n",
    "        scores += metrics.accuracy_score(y_cv, preds)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_cv, preds, pos_label=1)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        mean_auc += roc_auc\n",
    "        cm = metrics.confusion_matrix(y_cv, preds)\n",
    "        mean_tpr += float(cm[0][0]) / numpy.sum(cm[0])\n",
    "        cm0 += cm\n",
    "        prec, rec, _, _ = metrics.precision_recall_fscore_support(y_cv, preds, pos_label=1, average='binary')\n",
    "        mean_prec += numpy.mean(prec)\n",
    "        mean_rec += numpy.mean(rec)\n",
    "        mean_spec += rec\n",
    "        mean_f1 += 2 * numpy.mean(prec) * numpy.mean(rec) / (numpy.mean(prec) + numpy.mean(rec))\n",
    "\n",
    "        metrics_dict = {'Accuracy': float(scores),\n",
    "                    'AUC': float(mean_auc),\n",
    "                    'Sensitivity': float(mean_tpr),\n",
    "                    'Specificity': float(mean_spec),\n",
    "                    'Precision': float(mean_prec),\n",
    "                    'Recall': float(mean_rec),\n",
    "                    'F1-Score': float(mean_f1),\n",
    "                    'Confusion Matrix': str(cm0),\n",
    "                    'True Positive': int(cm0[1, 1]),\n",
    "                    'True Negative': int(cm0[0, 0]),\n",
    "                    'False Positive': int(cm0[0, 1]),\n",
    "                    'False Negative': int(cm0[1, 0])}\n",
    "        metrics_dict['Classifier'] = str(classifier).split('(')[0]\n",
    "#         metrics_dict['Threshold'] = threshold\n",
    "        pp.pprint(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
